---
apiVersion: v1
kind: ConfigMap
metadata:
  name: servo-config
data:
  servo.yaml: |
    kubernetes:
      namespace: default
      settlement: 5m
      deployments:
        - name: web
          strategy: canary
          replicas:
            min: 1
            max: 4
            step: 1
          containers:
          - name: main
            cpu:
              min: 500m
              max: 2.0
              step: 0.125
            memory:
              min: 128 MiB
              max: 3 GiB
              step: 0.125

    prometheus:
      # NOTE: In a sidecar configuration, Prometheus is colocated with the servo in the Pod
      base_url: http://localhost:9090
      metrics:
        - name: main_instance_count
          query: sum(envoy_cluster_membership_healthy{opsani_role!="tuning"})
          unit: count
        - name: tuning_instance_count
          query: envoy_cluster_membership_healthy{opsani_role="tuning"}
          unit: count

        - name: main_pod_avg_request_rate
          query: avg(rate(envoy_cluster_upstream_rq_total{opsani_role!="tuning"}[3m]))
          unit: rps
        - name: total_request_rate
          query: sum(rate(envoy_cluster_upstream_rq_total[3m]))
          unit: rps
        - name: main_request_rate
          query: sum(rate(envoy_cluster_upstream_rq_total{opsani_role!="tuning"}[3m]))
          unit: rps
        - name: tuning_request_rate
          query: rate(envoy_cluster_upstream_rq_total{opsani_role="tuning"}[3m])
          unit: rps

        - name: main_success_rate
          query: sum(rate(envoy_cluster_upstream_rq_xx{opsani_role!="tuning", envoy_response_code_class="2"}[3m]))
          unit: rps
        - name: tuning_success_rate
          query: rate(envoy_cluster_upstream_rq_xx{opsani_role="tuning", envoy_response_code_class="2"}[3m])
          unit: rps

        - name: main_error_rate
          query: sum(rate(envoy_cluster_upstream_rq_xx{opsani_role!="tuning", envoy_response_code_class=~"4|5"}[3m]))
          unit: rps
        - name: tuning_error_rate
          query: rate(envoy_cluster_upstream_rq_xx{opsani_role="tuning", envoy_response_code_class=~"4|5"}[3m])
          unit: rps

        - name: main_p90_latency
          query: avg(histogram_quantile(0.9,rate(envoy_cluster_upstream_rq_time_bucket{opsani_role!="tuning"}[3m])))
          unit: ms
        - name: tuning_p90_latency
          query: avg(histogram_quantile(0.9,rate(envoy_cluster_upstream_rq_time_bucket{opsani_role="tuning"}[3m])))
          unit: ms
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: servo
  labels:
    app: servo
    component: core
spec:
  replicas: 1
  revisionHistoryLimit: 2
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: servo
  template:
    metadata:
      name: servo
      labels:
        app: servo
        component: core
    spec:
      serviceAccountName: servo
      containers:
      - name: main
        image: opsani/servox:latest
        imagePullPolicy: Always
        env:
        - name: OPSANI_OPTIMIZER
          value: dev.opsani.com/blake-test-blank
        - name: SERVO_LOG_LEVEL
          value: DEBUG
        - name: POD_NAME
          valueFrom:
              fieldRef:
                fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        volumeMounts:
        - name: servo-token-volume
          mountPath: /servo/opsani.token
          subPath: opsani.token
          readOnly: true
        - name: servo-config-volume
          mountPath: /servo/servo.yaml
          subPath: servo.yaml
          readOnly: true
        resources:
          limits:
            cpu: 250m
            memory: 512Mi
      - name: prometheus
        image: quay.io/prometheus/prometheus:v2.20.1
        args:
          - '--storage.tsdb.retention.time=12h'
          - '--config.file=/etc/prometheus/prometheus.yaml'
        ports:
        - name: webui
          containerPort: 9090
        resources:
          requests:
            cpu: 100m
            memory: 128M
          limits:
            cpu: 500m
            memory: 1G
        volumeMounts:
        - name: prometheus-config-volume
          mountPath: /etc/prometheus
      volumes:
      - name: servo-token-volume
        secret:
          secretName: servo-token
          items:
          - key: token
            path: opsani.token
      - name: servo-config-volume
        configMap:
          name: servo-config
          items:
          - key: servo.yaml
            path: servo.yaml
      - name: prometheus-config-volume
        configMap:
          name: prometheus-config
---
apiVersion: v1
kind: Secret
metadata:
  name: servo-token
  namespace: default
  labels:
    app: servo
    component: core
type: Opaque
data:
  token: ZjRmYmMxNmI1ZWYzYjg3N2FhNDUyMmIyMzc1MDAyZDM3MTlhNGY4NDc1Zjc1MjBkZWZiYjIxCg==

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: servo
  labels:
    app: servo

---
# Cluster Role for the servo itself
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: servo
  labels:
    app: servo
    component: core
rules:
- apiGroups: ["apps","extensions"]
  resources: ["deployments","replicasets"]
  verbs: ["get", "list", "watch", "update", "patch"]
- apiGroups: [""]
  resources: ["pods", "pods/logs", "pods/status", "namespaces"]
  verbs: ["create", "delete", "get", "list", "watch" ]

---
# Cluster Role for the Prometheus sidecar
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
  labels:
    app: servo
    component: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["get"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]

---
# Bind the Servo Cluster Role to the servo Service Account
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: servo
  labels:
    app: servo
    component: core
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: servo
subjects:
- kind: ServiceAccount
  name: servo
  namespace: default

---
# Bind the Prometheus Cluster Role to the servo Service Account
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
  labels:
    app: servo
    component: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: servo
  namespace: default

---
### Begin Prometheus Configuration Manifests

apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  labels:
    app: servo
    component: prometheus
data:
  prometheus.yaml: |
    # Opsani Servo Prometheus Sidecar v0.6.2
    # This configuration allows the Opsani Servo to discover and scrape Pods that
    # have been injected with an Envoy proxy sidecar container that emits the metrics
    # necessary for optimization. Scraping by the Prometheus sidecar is enabled by
    # adding the following annotations to the Pod spec of the Deployment under
    # optimization:
    #
    # annotations:
    #   prometheus.opsani.com/path: /stats/prometheus # Default Envoy metrics path
    #   prometheus.opsani.com/port: "9901" # Default Envoy metrics port
    #   prometheus.opsani.com/scrape: "true" # Opt-in for scraping by the servo
    #
    # Path and port collisions with the optimization target can be resolved be changing
    # the relevant annotation.

    # Scrape the targets every 5 seconds.
    # Since we are only looking at specifically annotated Envoy sidecar containers
    # with a known metrics surface area and retain the values for <= 24 hours, we
    # can scrape aggressively. The higher scrape resolution is helpful for testing
    # and running checks that verify configuration health.
    global:
      scrape_interval: 5s
      scrape_timeout: 5s
      evaluation_interval: 5s

    # Scrape the Envoy sidecar metrics based on matching annotations (see above)
    scrape_configs:
    - job_name: 'opsani-envoy-sidecars'
      kubernetes_sd_configs:
        - role: pod
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_opsani_com_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_opsani_com_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_opsani_com_port]
          action: replace
          target_label: __address__
          regex: (.+)(?::\d+);(\d+)
          replacement: $1:$2
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
